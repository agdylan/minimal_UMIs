{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Data Notebook\n",
    "\n",
    "This notebook performs the following steps:\n",
    "\n",
    "- **Process the `bam_file`** for each dataset to generate a **counts DataFrame**.\n",
    "- **Construct AnnData matrices** from the counts DataFrame, representing the naive counts for \\( u[k] \\) across all \\( k \\).\n",
    "- **Generate predicted counts** using:\n",
    "  - The **uniform model**\n",
    "  - The **non-uniform model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing the Bam_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import sklearn\n",
    "import anndata as ad \n",
    "from tqdm import tqdm as tqdm\n",
    "import pysam\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import pandas as pd\n",
    "\n",
    "# 1) INPUT: Path to the filtered BAM file (choose one dataset)\n",
    "# filtered_bam_path = '/data/dagyeman/cellranger/bam_file_analysis/1k_PBMCs/filtered_1k_PBMCS_bam.bam'   # BAM file for 1k dataset\n",
    "filtered_bam_path = '/data/dagyeman/cellranger/bam_file_analysis/10k_PBMCs/filtered_10k_PBMCS_bam.bam' # BAM file for 10k dataset\n",
    "# filtered_bam_path = '/data/dagyeman/cellranger/bam_file_analysis/500_PBMCs/filtered_500_PBMCS_bam.bam'    # BAM file for 500 dataset\n",
    "# filtered_bam_path = '/data/dagyeman/cellranger/bam_file_analysis/5k_PBMCs/filtered_5k_PBMCS_bam.bam'    # BAM file for 5k dataset\n",
    "\n",
    "# 2) data: list to hold extracted (barcode, gene, UMI) triplets\n",
    "data = []\n",
    "\n",
    "# 3) EXTRACT: Read BAM file and pull out barcode, gene, and UMI tags\n",
    "with pysam.AlignmentFile(filtered_bam_path, \"rb\") as bam_file:\n",
    "    for read in bam_file:\n",
    "        if read.has_tag('CB') and read.has_tag('GN') and read.has_tag('UB'):\n",
    "            barcode = read.get_tag('CB')  # string: cell barcode\n",
    "            gene = read.get_tag('GN')     # string: gene name\n",
    "            umi = read.get_tag('UB')      # string: unique molecular identifier\n",
    "            data.append([barcode, gene, umi])\n",
    "\n",
    "# 4) df: pandas DataFrame containing all reads with columns (barcode, gene, UMI)\n",
    "df = pd.DataFrame(data, columns=['barcode', 'gene', 'UMI'])\n",
    "print(\"First few raw rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# 5) CLEAN: Remove any rows where UMI contains ambiguous base 'N'\n",
    "df = df[~df['UMI'].str.contains('N')]\n",
    "\n",
    "# 6) deduplicated_df: DataFrame containing only unique (barcode, gene, UMI) combinations\n",
    "#    This is the deduplicated set of observed UMIs before truncating\n",
    "deduplicated_df = df.drop_duplicates(subset=['barcode', 'gene', 'UMI']).reset_index(drop=True)\n",
    "print(\"First few deduplicated rows:\")\n",
    "print(deduplicated_df.head())\n",
    "\n",
    "# 7) LOOP: For UMI lengths k = 1 to 12, compute number of unique UMIs per (barcode, gene)\n",
    "for k in range(1, 13):\n",
    "    deduplicated_df[f'UMI_{k}'] = deduplicated_df['UMI'].str[:k]  # first k bases\n",
    "    grouped_df = (\n",
    "        deduplicated_df.groupby(['barcode', 'gene'])[f'UMI_{k}']\n",
    "                       .nunique()\n",
    "                       .reset_index()\n",
    "                       .rename(columns={f'UMI_{k}': f'unique_UMI_count_{k}'})\n",
    "    )\n",
    "    print(f\"Unique UMI counts for UMI length {k}:\")\n",
    "    print(grouped_df.head())\n",
    "\n",
    "# 8) final_df: merged table with unique UMI counts for all lengths 1–12\n",
    "final_df = (\n",
    "    deduplicated_df.groupby(['barcode', 'gene'])\n",
    "                   .agg({f'UMI_{k}': 'nunique' for k in range(1, 13)})\n",
    "                   .reset_index()\n",
    ")\n",
    "final_df.columns = ['barcode', 'gene'] + [f'unique_UMI_count_{k}' for k in range(1, 13)]\n",
    "print(\"Final merged table:\")\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adata Objects: Naive Method - created from final_df dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import anndata as ad\n",
    "\n",
    "# Dictionary to store the AnnData objects for each UMI length\n",
    "adata_dict = {}\n",
    "\n",
    "# Loop over UMI lengths from 1 to 12\n",
    "for k in range(1, 13):\n",
    "    # Extract the relevant columns for the current UMI length\n",
    "    # Contains: barcode, gene, and the deduplicated count for UMI length k\n",
    "    adata_matrix = final_df[['barcode', 'gene', f'unique_UMI_count_{k}']]\n",
    "\n",
    "    # Pivot the DataFrame:\n",
    "    # - Rows = barcodes (cells)\n",
    "    # - Columns = genes\n",
    "    # - Values = deduplicated UMI counts for the current UMI length\n",
    "    matrix_df = adata_matrix.pivot(\n",
    "        index='barcode',\n",
    "        columns='gene',\n",
    "        values=f'unique_UMI_count_{k}'\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Create an AnnData object from the counts matrix\n",
    "    adata = ad.AnnData(X=matrix_df.values)\n",
    "\n",
    "    # Assign observation names (rows) to the barcodes\n",
    "    adata.obs_names = matrix_df.index\n",
    "\n",
    "    # Assign variable names (columns) to the gene names\n",
    "    adata.var_names = matrix_df.columns\n",
    "\n",
    "    # Store the AnnData object in the dictionary with the UMI length as the key\n",
    "    adata_dict[k] = adata\n",
    "\n",
    "# Example: View the AnnData object for UMI length 1\n",
    "print(adata_dict[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving naive adata objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file = \"/data/dagyeman/cellranger/bam_file_analysis/1k_PBMCs/ub_objects/adata_matrices/\" # saving path for 1k dataset\n",
    "# filepath = \"/data/dagyeman/cellranger/bam_file_analysis/10k_PBMCs/ub_objects/adata_matrices/\" # saving path for 10k dataset \n",
    "\n",
    "# for k in range(1, 13):\n",
    "#     # Save each AnnData object to a file\n",
    "#     adata_dict[k].write_h5ad(f'{filepath}adata_UMI_length_{k}.h5ad')\n",
    "#     print(f'Saved AnnData object for UMI length {k} to adata_UMI_length_{k}.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adata Objects: Uniform Estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mom_estimator_unif(y, K):\n",
    "    \"\"\"\n",
    "    Method-of-moments estimator for the uniform UMI model.\n",
    "\n",
    "    Given observed unique molecule counts y and total possible UMIs K,\n",
    "    estimates the underlying molecule counts n using the closed-form\n",
    "    inversion for the uniform collision model.\n",
    "\n",
    "    Works element-wise on scalars, vectors, or matrices.\n",
    "    Special handling is applied when y == K using a recursive relation.\n",
    "    \"\"\"\n",
    "    # Make sure y is a NumPy array\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Prepare an output array of floats with the same shape as y\n",
    "    result = np.empty_like(y, dtype=float)\n",
    "    \n",
    "    # Handle entries where y equals K\n",
    "    special_mask = (y == K)\n",
    "    if special_mask.any():\n",
    "        # Special case value from recursive formula\n",
    "        special_value = mom_estimator_unif(K - 1, K) + K\n",
    "        result[special_mask] = special_value\n",
    "    \n",
    "    # Handle all other entries\n",
    "    general_mask = ~special_mask\n",
    "    if general_mask.any():\n",
    "        denominator = np.log(1 - 1/K)\n",
    "        result[general_mask] = np.log(1 - y[general_mask] / K) / denominator\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating AnnData matrix for uniform estimator at length k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 11458 × 33100\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "\n",
    "# filepath = \"/data/dagyeman/cellranger/bam_file_analysis/1k_PBMCs/ub_objects\" #1k\n",
    "filepath = \"/data/dagyeman/cellranger/bam_file_analysis/10k_PBMCs/ub_objects\" #10k\n",
    "\n",
    "# Load naive AnnData counts (UMI lengths 1–12)\n",
    "adata_dict = {}\n",
    "for i in range(1, 13):\n",
    "    adata_dict[i] = sc.read_h5ad(f\"{filepath}/adata_matrices/adata_{i}.h5ad\")\n",
    "    \n",
    "\n",
    "# Build the predicted AnnData for a chosen UMI length k \n",
    "chosen_k = 6  # set this as needed\n",
    "\n",
    "adata = adata_dict[chosen_k]                 # naive counts AnnData for k\n",
    "\n",
    "# Dense matrix view of counts\n",
    "matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "original_matrix = matrix.copy()              # keep a copy if you want to compare later\n",
    "\n",
    "# Predict counts with your estimator\n",
    "predicted_matrix = mom_estimator_unif(matrix, 4**chosen_k)\n",
    "# AnnData with predicted counts (cells x genes align with the naive object)\n",
    "predicted_adata = ad.AnnData(X=predicted_matrix)\n",
    "predicted_adata.obs_names = adata.obs_names\n",
    "predicted_adata.var_names = adata.var_names\n",
    "\n",
    "# Example check\n",
    "print(predicted_adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving adata matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scanpy as sc\n",
    "# import anndata as ad\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # filepath = \"/data/dagyeman/cellranger/bam_file_analysis/1k_PBMCs/ub_objects\"  # 1k\n",
    "# filepath = \"/data/dagyeman/cellranger/bam_file_analysis/10k_PBMCs/ub_objects\"   # 10k\n",
    "\n",
    "# # Load naive AnnData counts (UMI lengths 1–12)\n",
    "# adata_dict = {}\n",
    "# for i in range(1, 13):\n",
    "#     adata_dict[i] = sc.read_h5ad(f\"{filepath}/adata_matrices/adata_{i}.h5ad\")\n",
    "\n",
    "# # Output directory for uniform-estimator predictions\n",
    "# save_dir = f\"{filepath}/inv_unif_matrices\"\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Build and save predicted AnnData for each UMI length k\n",
    "# for k in range(1, 13):\n",
    "#     adata = adata_dict[k]\n",
    "#     matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "\n",
    "#     predicted_matrix = mom_estimator_unif(matrix, 4**k)\n",
    "\n",
    "#     predicted_adata = ad.AnnData(X=predicted_matrix)\n",
    "#     predicted_adata.obs_names = adata.obs_names\n",
    "#     predicted_adata.var_names = adata.var_names\n",
    "\n",
    "#     out_path = os.path.join(save_dir, f\"adata_{k}.h5ad\")\n",
    "#     predicted_adata.write_h5ad(out_path)\n",
    "#     print(f\"Saved: {out_path}\")\n",
    "\n",
    "# # Example check\n",
    "# print(sc.read_h5ad(os.path.join(save_dir, \"adata_6.h5ad\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adata Objects: Non-uniform Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "@njit\n",
    "def interpolate(y, f_hat_values, n_values):\n",
    "    \"\"\"Interpolates the value of n for a given y using linear interpolation.\"\"\"\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    # Find index in f_hat_values where y fits in\n",
    "    i = np.searchsorted(f_hat_values, y) - 1\n",
    "    # Clamp i so it’s never out of range\n",
    "    i = max(0, min(i, len(f_hat_values) - 2))\n",
    "\n",
    "    # Values for n and f at positions i and i+1\n",
    "    n_i = n_values[i]\n",
    "    n_ip1 = n_values[i + 1]\n",
    "    f_ni = f_hat_values[i]\n",
    "    f_nip1 = f_hat_values[i + 1]\n",
    "\n",
    "    # Linear interpolation\n",
    "    denom = f_nip1 - f_ni\n",
    "    return n_i + (y - f_ni) * (n_ip1 - n_i) / denom\n",
    "\n",
    "@njit(parallel=True)\n",
    "def interpolate_array(matrix_flat, f_hat_values, n_values, j):\n",
    "    \"\"\"Applies interpolation to each element in a flattened matrix.\"\"\"\n",
    "    result = np.empty_like(matrix_flat)\n",
    "    target_value = 4 ** j\n",
    "\n",
    "    for idx in prange(matrix_flat.size):\n",
    "        y = matrix_flat[idx]\n",
    "        if y == target_value:\n",
    "            # Special handling if value equals theoretical max\n",
    "            result[idx] = (\n",
    "                3 * interpolate(y - 1, f_hat_values, n_values)\n",
    "                - 3 * interpolate(y - 2, f_hat_values, n_values)\n",
    "                + interpolate(y - 3, f_hat_values, n_values)\n",
    "            )\n",
    "        else:\n",
    "            result[idx] = interpolate(y, f_hat_values, n_values)\n",
    "    return result\n",
    "\n",
    "def mom_estimator_nonunif(prob_arr, matrix, j, n_max=15000, grow=1.33, cap=2_000_000, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Non-uniform method-of-moments inversion.\n",
    "    Adaptive growth of n_max is only applied when j >= 7.\n",
    "    \"\"\"\n",
    "\n",
    "    matrix_flat = matrix.flatten().astype(np.float64)\n",
    "    target = 4 ** j  # theoretical max UMIs\n",
    "\n",
    "    # Largest y we need to cover (ignore exact target)\n",
    "    if (matrix_flat != target).any():\n",
    "        y_needed = float(matrix_flat[matrix_flat != target].max())\n",
    "    else:\n",
    "        y_needed = 0.0\n",
    "\n",
    "    if j >= 7:\n",
    "        # Adaptive growth loop\n",
    "        while True:\n",
    "            n_values = np.arange(1, n_max + 1, dtype=np.float64)\n",
    "            f_hat_values = np.array(\n",
    "                [target - np.sum((1.0 - prob_arr) ** n) for n in n_values],\n",
    "                dtype=np.float64\n",
    "            )\n",
    "\n",
    "            f_min = f_hat_values[0]\n",
    "            f_max = f_hat_values[-1]\n",
    "\n",
    "            if f_max + eps >= y_needed or y_needed <= f_min + eps:\n",
    "                break\n",
    "\n",
    "            if n_max >= cap:\n",
    "                raise ValueError(\n",
    "                    f\"n_max reached cap ({cap}) while y_needed={y_needed:.6f} > f_max={f_max:.6f} for j={j}.\"\n",
    "                )\n",
    "            n_max = min(cap, int(round(n_max * grow)))\n",
    "    else:\n",
    "        # For small j, just compute once\n",
    "        n_values = np.arange(1, n_max + 1, dtype=np.float64)\n",
    "        f_hat_values = np.array(\n",
    "            [target - np.sum((1.0 - prob_arr) ** n) for n in n_values],\n",
    "            dtype=np.float64\n",
    "        )\n",
    "        f_min, f_max = f_hat_values[0], f_hat_values[-1]\n",
    "\n",
    "    # Final coverage: if anything is still out of range, keep growing here\n",
    "    while True:\n",
    "        too_high = (matrix_flat > f_max + eps) & (matrix_flat != target)\n",
    "        too_low  = (matrix_flat < f_min - eps) & (matrix_flat != 0.0)\n",
    "\n",
    "        if not (too_high.any() or too_low.any()):\n",
    "            break\n",
    "\n",
    "        if j < 7:\n",
    "            raise ValueError(\n",
    "                f\"Observed y outside interpolation range [{f_min:.6f}, {f_max:.6f}] \"\n",
    "                f\"(excluding 0 and {target}) with j<{7}.\"\n",
    "            )\n",
    "\n",
    "        if n_max >= cap:\n",
    "            raise ValueError(\n",
    "                f\"Observed y outside interpolation range [{f_min:.6f}, {f_max:.6f}] \"\n",
    "                f\"(excluding 0 and {target}), and n_max hit cap {cap}.\"\n",
    "            )\n",
    "\n",
    "        n_max = min(cap, int(round(n_max * grow)))\n",
    "        n_values = np.arange(1, n_max + 1, dtype=np.float64)\n",
    "        f_hat_values = np.array(\n",
    "            [target - np.sum((1.0 - prob_arr) ** n) for n in n_values],\n",
    "            dtype=np.float64\n",
    "        )\n",
    "        f_min, f_max = f_hat_values[0], f_hat_values[-1]\n",
    "\n",
    "    interpolated_flat = interpolate_array(matrix_flat, f_hat_values, n_values, j)\n",
    "    return interpolated_flat.reshape(matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating AnnData matrix for non-uniform estimator at length k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 11458 × 33100\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "\n",
    "# filepath = \"/data/dagyeman/cellranger/bam_file_analysis/1k_PBMCs/ub_objects\" #1k\n",
    "filepath = \"/data/dagyeman/cellranger/bam_file_analysis/10k_PBMCs/ub_objects\" #10k\n",
    "\n",
    "# Load naive AnnData counts (UMI lengths 1–12)\n",
    "adata_dict = {}\n",
    "for i in range(1, 13):\n",
    "    adata_dict[i] = sc.read_h5ad(f\"{filepath}/adata_matrices/adata_{i}.h5ad\")\n",
    "\n",
    "# Load non-uniform UMI probability distributions (UMI lengths 1–12)\n",
    "umi_prob_dict = {}\n",
    "for i in range(1, 13):\n",
    "    umi_prob_dict[i] = pd.read_csv(f\"/data/dagyeman/cellranger/bam_file_analysis/data/umi_probs/umi_probs_{i}.csv\")\n",
    "\n",
    "# Build the predicted AnnData for a chosen UMI length k\n",
    "chosen_k = 1  # set this as needed\n",
    "\n",
    "adata = adata_dict[chosen_k]                 # naive counts AnnData for k\n",
    "umi_probs = umi_prob_dict[chosen_k]['prob']  # non-uniform probs for k\n",
    "\n",
    "# Dense matrix view of counts\n",
    "matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "original_matrix = matrix.copy()              # keep a copy if you want to compare later\n",
    "\n",
    "# Predict counts with your estimator\n",
    "predicted_matrix = mom_estimator_nonunif(umi_probs.values, matrix, j=chosen_k)\n",
    "\n",
    "# AnnData with predicted counts (cells x genes align with the naive object)\n",
    "predicted_adata = ad.AnnData(X=predicted_matrix)\n",
    "predicted_adata.obs_names = adata.obs_names\n",
    "predicted_adata.var_names = adata.var_names\n",
    "\n",
    "# Example check\n",
    "print(predicted_adata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving inv_nonuniform matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11458, 33100)\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# filepath = \"/data/dagyeman/cellranger/bam_file_analysis/1k_PBMCs/ub_objects\"  # 1k\n",
    "filepath = \"/data/dagyeman/cellranger/bam_file_analysis/10k_PBMCs/ub_objects\"   # 10k\n",
    "\n",
    "# Load naive AnnData counts (UMI lengths 1–12)\n",
    "adata_dict = {}\n",
    "for i in range(1, 13):\n",
    "    adata_dict[i] = sc.read_h5ad(f\"{filepath}/adata_matrices/adata_{i}.h5ad\")\n",
    "    \n",
    "print(adata_dict[1].shape)\n",
    "\n",
    "# Load non-uniform UMI probability distributions (UMI lengths 1–12)\n",
    "umi_prob_dict = {}\n",
    "for i in range(1, 13):\n",
    "    umi_prob_dict[i] = pd.read_csv(f\"/data/dagyeman/cellranger/bam_file_analysis/data/umi_probs/umi_probs_{i}.csv\")\n",
    "\n",
    "# Output directory for non-uniform estimator predictions\n",
    "save_dir = f\"{filepath}/inv_nonunif_matrices\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Build and save predicted AnnData for each UMI length k\n",
    "for k in range(12, 13):\n",
    "    adata = adata_dict[k]\n",
    "    umi_probs = umi_prob_dict[k]['prob'].values\n",
    "\n",
    "    matrix = adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X\n",
    "    predicted_matrix = mom_estimator_nonunif(umi_probs, matrix, j=k)\n",
    "\n",
    "    predicted_adata = ad.AnnData(X=predicted_matrix)\n",
    "    predicted_adata.obs_names = adata.obs_names\n",
    "    predicted_adata.var_names = adata.var_names\n",
    "\n",
    "    out_path = os.path.join(save_dir, f\"adata_{k}.h5ad\")\n",
    "    predicted_adata.write_h5ad(out_path)\n",
    "    print(f\"Saved: {out_path}\")\n",
    "\n",
    "# Example check\n",
    "# print(sc.read_h5ad(os.path.join(save_dir, \"adata_6.h5ad\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16160.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read_h5ad(\"/data/dagyeman/cellranger/bam_file_analysis/10k_PBMCs/ub_objects/adata_matrices/adata_12.h5ad\")\n",
    "max(adata.X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
